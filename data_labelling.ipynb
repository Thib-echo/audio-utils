{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import spacy\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from unidecode import unidecode\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "DATA_FOLDER = Path(\"../audio_database/big_db_test/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify and save personnal information from transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_name(name):\n",
    "    \"\"\"Check if the name is valid for matching.\"\"\"\n",
    "    cleaned_name = name.strip()\n",
    "        # Exclude names that are too short\n",
    "    if len(cleaned_name) <= 2:\n",
    "        return False\n",
    "    \n",
    "    # Exclude specific non-name words or symbols\n",
    "    non_name_keywords = [\"sos\", \"oui\", \"allô\", \"médecin\", \"médecins\", \"bonjour\", \"covid\"]  # Add more keywords as needed\n",
    "    if cleaned_name.lower() in non_name_keywords:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def find_names_with_spacy(transcript):\n",
    "    nlp = spacy.load('fr_dep_news_trf')\n",
    "    doc = nlp(transcript)\n",
    "    names = list(set([w.text for w in doc if w.pos_ == 'PROPN' and is_valid_name(w.text)]))\n",
    "    \n",
    "    return names\n",
    "\n",
    "def month_name_to_number(month_name):\n",
    "    month_names = ['janvier', 'février', 'mars', 'avril', 'mai', 'juin', \n",
    "                   'juillet', 'août', 'septembre', 'octobre', 'novembre', 'décembre']\n",
    "    try:\n",
    "        return month_names.index(month_name.lower()) + 1\n",
    "    except ValueError:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def extract_full_date(transcript):\n",
    "    min_year = 1900\n",
    "\n",
    "    # Pattern for numerical full date\n",
    "    full_date_pattern = r'\\b(0?[1-9]|[12][0-9]|3[01])[-/](0?[1-9]|1[0-2])[-/](\\d{2}|\\d{4})\\b'\n",
    "    \n",
    "    # Pattern for full date with month name\n",
    "    month_names = '|'.join(['janvier', 'février', 'mars', 'avril', 'mai', 'juin', \n",
    "                            'juillet', 'août', 'septembre', 'octobre', 'novembre', 'décembre'])\n",
    "    date_with_month_pattern = r'\\b(0?[1-9]|[12][0-9]|3[01])\\s+(' + month_names + r')\\s+(\\d{4})\\b'\n",
    "\n",
    "    # Try to extract full date in numerical format\n",
    "    full_dates = re.findall(full_date_pattern, transcript)\n",
    "    formatted_dates = []\n",
    "    if full_dates:\n",
    "        for date in full_dates:\n",
    "            day, month, year = date\n",
    "            if len(year) == 2:  # Convert two-digit year to four digits\n",
    "                year = \"19\" + year if year > \"24\" else \"20\" + year\n",
    "            # Handle transcript erros (ex: 1568 instead of 1968)\n",
    "\n",
    "            if int(year) < min_year:\n",
    "                year = int(\"19\" + year[-2:])\n",
    "\n",
    "            formatted_dates.append(f\"{day.zfill(2)}/{month.zfill(2)}/{str(year)}\")\n",
    "    \n",
    "        return formatted_dates\n",
    "\n",
    "    # Try to extract full date with month name\n",
    "    dates_with_month = re.findall(date_with_month_pattern, transcript)\n",
    "    if dates_with_month:\n",
    "        for date in dates_with_month:\n",
    "            day, month_name, year = date\n",
    "            month_number = month_name_to_number(month_name)\n",
    "            if month_number:\n",
    "                formatted_dates.append(f\"{day.zfill(2)}/{month_number:02d}/{year}\")\n",
    "        \n",
    "        return formatted_dates\n",
    "\n",
    "    # Extract just the year\n",
    "    year_pattern = r'\\b(19\\d{2}|20\\d{2})\\b'\n",
    "    years = re.findall(year_pattern, transcript)\n",
    "    if years:\n",
    "        year = int(years[0])\n",
    "        if year < min_year:\n",
    "            year = int(\"19\" + year[-2:])\n",
    "        return f\"01/01/{str(year)}\"  # Defaulting to '01/01/YYYY' for year-only cases\n",
    "\n",
    "    return None\n",
    "\n",
    "def extract_phone_numbers(transcript):\n",
    "    phone_regex = r'(06|07)(\\d{8})'\n",
    "\n",
    "    transcript = transcript.replace(\" \", \"\")\n",
    "    transcript = transcript.translate(str.maketrans('', '', string.punctuation))\n",
    "    match = re.search(phone_regex, transcript)\n",
    "    match = match.group() if match else match\n",
    "    return match\n",
    "\n",
    "def extract_postcodes(transcript):\n",
    "    # Existing pattern for postcodes like '75015' or '75-015'\n",
    "    postcode_pattern = r'\\b(75|77|78|91|92|93|94|95)(?:[-\\s]?)(\\d{3})\\b'\n",
    "    raw_postcodes = re.findall(postcode_pattern, transcript)\n",
    "\n",
    "    # Additional pattern for postcodes written as '19ème'\n",
    "    arrondissement_pattern = r'\\b(\\d{1,2})[eè]me\\b'\n",
    "    arrondissements = re.findall(arrondissement_pattern, transcript)\n",
    "\n",
    "    # Formatting arrondissements to Paris postcodes (e.g., '75019' for '19ème')\n",
    "    formatted_arrondissements = ['75' + arr.zfill(3) for arr in arrondissements]\n",
    "\n",
    "    # Combining and returning all found postcodes\n",
    "    return [''.join(match) for match in raw_postcodes] + formatted_arrondissements\n",
    "\n",
    "\n",
    "def normalize_transcript(transcript):\n",
    "    # Define a mapping of abbreviations to expansions\n",
    "    abbreviations = {\n",
    "        \" dr \": \" docteur \",\n",
    "        \" st \": \" saint \",\n",
    "        \" ste \": \" sainte \",\n",
    "        # Add more abbreviations and their expansions as needed\n",
    "    }\n",
    "    # Ensure the address is a lowercase string\n",
    "    normalized_transcript = unidecode(str(transcript).lower())\n",
    "    # Replace abbreviations with their expanded form\n",
    "    for abbr, expansion in abbreviations.items():\n",
    "        normalized_transcript = normalized_transcript.replace(abbr, expansion)\n",
    "    # Replace hyphens with spaces\n",
    "    normalized_transcript = normalized_transcript.replace(\"-\", \" \")\n",
    "    return normalized_transcript\n",
    "\n",
    "def clean_trailing_words(address):\n",
    "    stop_words = ['dans le', \"c'est ca\", \" alors \", \" a \", \" et \", \" avec \", '.', ',', '!', '?']\n",
    "\n",
    "    for stop_word in stop_words:\n",
    "        if stop_word in address:\n",
    "            # Remove the stop word and anything that follows\n",
    "            address = address.split(stop_word)[0]\n",
    "            break\n",
    "    return address.strip()\n",
    "\n",
    "def extract_personal_info(data_folder):\n",
    "    personal_info = {}\n",
    "\n",
    "    # address_pattern = r'\\b(\\d+\\s+(?:rue|rues|boulevard|avenue|place)\\s+[a-zA-Zéèàêûôâîç\\s-]+)\\b'\n",
    "    address_pattern = r'\\b(\\d+\\s+(?:rue|rues|boulevard|avenue|place|square|villa|quai|allée|chaussée|passage)\\s+[a-zA-Zéèàêûôâîç\\s-]+(?:\\'[a-zA-Zéèàêûôâîç\\s-]+)?)\\b'\n",
    "\n",
    "    # address_pattern = r'\\b\\d+\\s*(rue|rues|avenue|boulevard|place)\\s*[^\\d,]+\\b'\n",
    "    number_sequence_pattern = r'\\b\\d+\\b'\n",
    "\n",
    "    for file_path in tqdm(data_folder.rglob('*.txt')):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            transcript = file.read()\n",
    "            transcript = normalize_transcript(transcript)\n",
    "\n",
    "            personal_info[str(Path(file_path.name))] = {\n",
    "                'date_of_birth': {\n",
    "                    'full_date': extract_full_date(transcript)\n",
    "                },\n",
    "                'addresses': list(set([clean_trailing_words(address) for address in re.findall(address_pattern, transcript)])),\n",
    "                'postcodes': extract_postcodes(transcript),\n",
    "                'phone_numbers': extract_phone_numbers(transcript),\n",
    "                # 'names' : list(set(find_names_with_spacy(transcript))),\n",
    "                # 'other_findings': re.findall(number_sequence_pattern, transcript),\n",
    "            }\n",
    "        \n",
    "    return personal_info\n",
    "\n",
    "personnal_data = extract_personal_info(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_personal_data_stats(personal_data):\n",
    "    stats = {\n",
    "        'total_files': 0,\n",
    "        'non_empty_files': 0,\n",
    "        'date_of_birth': 0,\n",
    "        'addresses': 0,\n",
    "        'postcodes': 0,\n",
    "        'phone_numbers': 0\n",
    "    }\n",
    "\n",
    "    for _, info in personal_data.items():\n",
    "        stats['total_files'] += 1\n",
    "        non_empty = False\n",
    "\n",
    "        for key, value in info.items():\n",
    "            # Special handling for 'date_of_birth' as it contains a nested dictionary\n",
    "            if key == 'date_of_birth':\n",
    "                if value.get('full_date'):  # Check if 'full_date' is not empty\n",
    "                    non_empty = True\n",
    "                    stats[key] += 1\n",
    "            elif value:  # Checks if the item is non-empty for other keys\n",
    "                non_empty = True\n",
    "                stats[key] += len(value) if isinstance(value, list) else 1\n",
    "\n",
    "        if non_empty:\n",
    "            stats['non_empty_files'] += 1\n",
    "\n",
    "    return stats\n",
    "\n",
    "# Usage\n",
    "stats = calculate_personal_data_stats(personnal_data)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_missing_keys(personnal_data, label_csv):\n",
    "    missed_matches = {}\n",
    "    # Get the unique values in the \"file_path\" column of label_csv\n",
    "    label_file_paths = set(label_csv[\"file_path\"])\n",
    "\n",
    "    # Iterate over the keys in the personnal_data dictionary\n",
    "    for key, info in personnal_data.items():\n",
    "        # Check if the key is not present in the label_file_paths set\n",
    "        if key not in label_file_paths and info['addresses']:\n",
    "            print(info['addresses'])\n",
    "            missed_matches[key] = info\n",
    "    return missed_matches\n",
    "\n",
    "label_csv = pd.read_csv(\"./output_labels.csv\", sep=',', encoding='utf-8')\n",
    "\n",
    "missed_matches = print_missing_keys(personnal_data, label_csv)\n",
    "print(len(missed_matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "from fuzzywuzzy import process, fuzz\n",
    "import re\n",
    "\n",
    "def extract_road_number(address):\n",
    "    match = re.search(r'\\b\\d+\\b', address)\n",
    "    return match.group(0) if match else None\n",
    "\n",
    "def road_numbers_match(csv_road_number, input_road_number):\n",
    "    # If the CSV address has 'nan' for the road number, we treat it as a match\n",
    "    if csv_road_number == 'nan':\n",
    "        return True\n",
    "    # Check if the input road number is contained within the CSV road number or vice versa\n",
    "    return input_road_number in csv_road_number or csv_road_number in input_road_number\n",
    "\n",
    "def find_matching_row(csv_path, data_dict):\n",
    "    # Define the columns to read\n",
    "    cols_to_use = ['Date', 'Annee', 'Mois', 'Jour', 'Heure', 'Min', 'FullAdresse', 'TelPatient', 'DateNaiss', 'CodePostal', 'Nom', 'Prenom', 'Devenir', 'NumRue']\n",
    "\n",
    "    # Read the CSV just once\n",
    "    df = pd.read_csv(csv_path, usecols=cols_to_use, sep=',', encoding='utf-8')\n",
    "    df.dropna(subset=['Devenir'],inplace=True)\n",
    "\n",
    "    df.fillna({'Annee': 0, 'Mois': 0, 'Jour': 0, 'Heure': 0, 'Min': 0, 'Sec': 0}, inplace=True)\n",
    "    df['DateTime'] = pd.to_datetime(df['Annee'].astype(int).astype(str) + '-' +\n",
    "                                    df['Mois'].astype(int).astype(str).str.zfill(2) + '-' +\n",
    "                                    df['Jour'].astype(int).astype(str).str.zfill(2) + ' ' +\n",
    "                                    df['Heure'].astype(int).astype(str).str.zfill(2) + ':' +\n",
    "                                    df['Min'].astype(int).astype(str).str.zfill(2),\n",
    "                                    errors='coerce')\n",
    "    df['DateNaiss'] = pd.to_datetime(df['DateNaiss'], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "    # Preprocess 'FullAdresse'\n",
    "    df['FullAdresse'] = df['FullAdresse'].apply(lambda x: unidecode(str(x).lower()).replace(\"-\", \" \"))\n",
    "\n",
    "    # Prepare results dictionary\n",
    "    results = {}\n",
    "\n",
    "    for file_path, info in tqdm(data_dict.items()):\n",
    "        # Extract and convert full datetime from file_path\n",
    "        datetime_parts = file_path.split('_')[:6]\n",
    "        datetime_str = '-'.join(datetime_parts[:3]) + ' ' + ':'.join(datetime_parts[3:6])\n",
    "        file_datetime = pd.to_datetime(datetime_str)\n",
    "\n",
    "        # Calculate 4-hour window\n",
    "        end_time = file_datetime + timedelta(hours=2)\n",
    "\n",
    "        # Filter the dataframe by the 4-hour window\n",
    "        date_filtered_df = df[(df['DateTime'] >= file_datetime) & (df['DateTime'] <= end_time)]\n",
    "\n",
    "        if info.get('phone_numbers'):\n",
    "            transcript_phone_number = info['phone_numbers']\n",
    "            medical_db_phone_number = date_filtered_df['TelPatient'].astype('str').str.contains(transcript_phone_number, na=False)\n",
    "            match = date_filtered_df[medical_db_phone_number]\n",
    "            if not match.empty:\n",
    "                results[file_path] = (match, \"phone_numbers\")\n",
    "                continue\n",
    "\n",
    "        if info.get('addresses'):\n",
    "            for address in info['addresses']:\n",
    "                list_adresse = date_filtered_df['FullAdresse'].tolist()\n",
    "                best_match = process.extractOne(address, list_adresse, scorer=fuzz.token_sort_ratio)\n",
    "                if best_match:\n",
    "                    # print(file_path, address, best_match)\n",
    "\n",
    "                    # If we have a perfect match, we're done\n",
    "                    if best_match[1] == 100:\n",
    "                        match = date_filtered_df[date_filtered_df['FullAdresse'].str.contains(re.escape(best_match[0]), na=False)]\n",
    "                        if not match.empty:\n",
    "                            results[file_path] = (match, \"addresses\")\n",
    "                            break\n",
    "\n",
    "                    # For high-scoring matches, we check the road numbers\n",
    "                    elif best_match[1] > 80:\n",
    "                        input_road_number = extract_road_number(address)\n",
    "                        csv_road_number = extract_road_number(best_match[0]) or 'nan'  # Treat missing numbers as 'nan'\n",
    "                        if road_numbers_match(csv_road_number, input_road_number):\n",
    "                            match = date_filtered_df[date_filtered_df['FullAdresse'].str.contains(re.escape(best_match[0]), na=False)]\n",
    "                            if not match.empty:\n",
    "                                results[file_path] = (match, \"addresses\")\n",
    "                                break\n",
    "\n",
    "        if info.get('date_of_birth', {}).get('full_date'):\n",
    "            for dob in info['date_of_birth']['full_date']:\n",
    "                dob = pd.to_datetime(dob, dayfirst=True, errors='coerce')\n",
    "                if dob is not pd.NaT:\n",
    "                    match = date_filtered_df[date_filtered_df['DateNaiss'] == dob]\n",
    "                    if not match.empty:\n",
    "                        results[file_path] = (match, \"date_of_birth\")\n",
    "                        continue\n",
    "\n",
    "        if info.get('postcodes'):\n",
    "            for postcode in info['postcodes']:\n",
    "                match = date_filtered_df[date_filtered_df['CodePostal'] == str(postcode)]\n",
    "                if not match.empty:\n",
    "                    results[file_path] = (match, \"postcodes\")\n",
    "                    break\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "medical_db_path = \"../audio_database/base_medicale//BaseMed2022LR_cleaned.csv\"\n",
    "\n",
    "matches = find_matching_row(medical_db_path, missed_matches)\n",
    "print(len(matches))\n",
    "# for file_path, matched_rows in matches.items():\n",
    "#     print(f\"File: {file_path}\")\n",
    "#     print(matched_rows[0][\"IdAppel\"], matched_rows[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_csv = pd.read_csv(\"./output_labels.csv\", sep=',', encoding='utf-8')\n",
    "\n",
    "print(label_csv.value_counts('Devenir'))\n",
    "\n",
    "# Count the number of samples for each value of Devenir\n",
    "count_0 = len(label_csv[label_csv['Devenir'] == 0])\n",
    "count_1 = len(label_csv[label_csv['Devenir'] == 1])\n",
    "\n",
    "# Determine the minimum number of samples to be selected for each value\n",
    "min_samples = min(count_0, count_1)\n",
    "\n",
    "# Calculate the desired number of samples for Devenir == 0 and Devenir == 1\n",
    "desired_count_0 = min_samples * 3\n",
    "desired_count_1 = min_samples\n",
    "\n",
    "# Sample the data based on the desired counts\n",
    "sampled_df_0 = label_csv[label_csv['Devenir'] == 0].sample(n=desired_count_0, replace=True)\n",
    "sampled_df_1 = label_csv[label_csv['Devenir'] == 1].sample(n=desired_count_1, replace=True)\n",
    "\n",
    "# Concatenate the sampled dataframes\n",
    "sampled_df = pd.concat([sampled_df_0, sampled_df_1])\n",
    "\n",
    "# Reset the index of the sampled dataframe\n",
    "sampled_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "sampled_df.to_csv('sampled_data_1to3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"On réussi a matcher {len(matches)} fichiers sur {stats['total_files']} au total, soit : {len(matches) / stats['total_files'] * 100:.02f} % de la base\")\n",
    "print(f\"On réussi a matcher {len(matches)} fichiers sur {stats['non_empty_files']} fichier ayant des infos personnelles récupèrable, soit : {len(matches) / stats['non_empty_files'] * 100:.02f} % de la base\")\n",
    "print(f\"On réussi a matcher {len(matches)} fichiers sur {stats['addresses']} fichier avec adresse récupèrable, soit : {len(matches) / stats['addresses'] * 100:.02f} % de la base\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
